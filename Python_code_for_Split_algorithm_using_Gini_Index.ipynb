{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOundldF3t1Wj66baC9njp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JinxWycman/MACHINE-LEARNING/blob/main/Python_code_for_Split_algorithm_using_Gini_Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFS6zq4_PlFC",
        "outputId": "d73b73f8-f731-41f4-d89c-15c227dd8bd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree:\n",
            "{'feature_1': {0: {'feature_2': {0: 'B', 'other': 'B'}}, 'other': {'feature_2': {0: 'A', 'other': 'A'}}}}\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "# Define a toy dataset (features and labels)\n",
        "data = [\n",
        "    {'feature_1': 1, 'feature_2': 0, 'label': 'A'},\n",
        "    {'feature_1': 0, 'feature_2': 1, 'label': 'B'},\n",
        "    {'feature_1': 1, 'feature_2': 1, 'label': 'A'},\n",
        "    {'feature_1': 0, 'feature_2': 0, 'label': 'B'},\n",
        "    {'feature_1': 1, 'feature_2': 0, 'label': 'B'},\n",
        "    {'feature_1': 0, 'feature_2': 1, 'label': 'A'},\n",
        "    {'feature_1': 1, 'feature_2': 1, 'label': 'B'},\n",
        "    {'feature_1': 0, 'feature_2': 0, 'label': 'A'},\n",
        "]\n",
        "\n",
        "# Calculate Gini index for a dataset\n",
        "def calculate_gini(data):\n",
        "    total_count = len(data)\n",
        "    if total_count == 0:  # Handle empty dataset case\n",
        "        return 0.0\n",
        "    label_counts = {}\n",
        "    for record in data:\n",
        "        label = record['label']\n",
        "        if label not in label_counts:\n",
        "            label_counts[label] = 0\n",
        "        label_counts[label] += 1\n",
        "\n",
        "    gini_index = 1.0\n",
        "    for label in label_counts:\n",
        "        probability = label_counts[label] / total_count\n",
        "        gini_index -= probability ** 2\n",
        "    return gini_index\n",
        "\n",
        "# Split the dataset based on a feature and value\n",
        "def split_dataset(data, feature, value):\n",
        "    left = [record for record in data if record[feature] == value]\n",
        "    right = [record for record in data if record[feature] != value]\n",
        "    return left, right\n",
        "\n",
        "# Select the best split based on Gini index\n",
        "def select_best_split(data, features):\n",
        "    best_gini = 1.0\n",
        "    best_split = None\n",
        "\n",
        "    for feature in features:\n",
        "        values = set(record[feature] for record in data)\n",
        "        for value in values:\n",
        "            left, right = split_dataset(data, feature, value)\n",
        "\n",
        "            # Avoid splits that result in empty datasets\n",
        "            if not left or not right:\n",
        "                continue\n",
        "\n",
        "            gini_left = calculate_gini(left)\n",
        "            gini_right = calculate_gini(right)\n",
        "\n",
        "            # Calculate weighted Gini index\n",
        "            weighted_gini = (len(left) / len(data)) * gini_left + (len(right) / len(data)) * gini_right\n",
        "\n",
        "            # Select the split with the lowest weighted Gini index\n",
        "            if weighted_gini < best_gini:\n",
        "                best_gini = weighted_gini\n",
        "                best_split = (feature, value, gini_left, gini_right)\n",
        "\n",
        "    return best_split\n",
        "\n",
        "# Recursive function to build the decision tree\n",
        "def build_tree(data, features):\n",
        "    # Base case 1: If all data points have the same label, return the label\n",
        "    if len(set(record['label'] for record in data)) == 1:\n",
        "        return data[0]['label']\n",
        "\n",
        "    # Base case 2: If there are no features left to split on, return the majority label\n",
        "    if len(features) == 0:\n",
        "        # Find the majority label\n",
        "        label_counts = {}\n",
        "        for record in data:\n",
        "            label = record['label']\n",
        "            label_counts[label] = label_counts.get(label, 0) + 1\n",
        "        return max(label_counts, key=label_counts.get)\n",
        "\n",
        "    # Select the best split\n",
        "    best_split = select_best_split(data, features)\n",
        "\n",
        "    # Base case 3: If no good split is found, return the majority label\n",
        "    if not best_split:\n",
        "        label_counts = {}\n",
        "        for record in data:\n",
        "            label = record['label']\n",
        "            label_counts[label] = label_counts.get(label, 0) + 1\n",
        "        return max(label_counts, key=label_counts.get)\n",
        "\n",
        "\n",
        "    feature, value, _, _ = best_split\n",
        "\n",
        "    # Build the tree branches recursively\n",
        "    tree = {feature: {}}\n",
        "    left, right = split_dataset(data, feature, value)\n",
        "\n",
        "    # Create copies of the features list for recursive calls\n",
        "    remaining_features = features[:]\n",
        "    remaining_features.remove(feature)\n",
        "\n",
        "    tree[feature][value] = build_tree(left, remaining_features)\n",
        "    tree[feature]['other'] = build_tree(right, remaining_features)\n",
        "\n",
        "    return tree\n",
        "\n",
        "# Main function to build and print the decision tree\n",
        "def main():\n",
        "    features = list(data[0].keys())\n",
        "    features.remove('label')\n",
        "    tree = build_tree(data, features)\n",
        "    print(\"Decision Tree:\")\n",
        "    print(tree)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 4.9: Output of code 4.6\n",
        "This example demonstrates how to implement a binary decision tree using the Gini index for splitting the data into subsets based on feature values. The decision tree is built recursively by selecting the best split at each node, and it can be used for classification tasks.\n",
        "**bold text**"
      ],
      "metadata": {
        "id": "iynFkEnwP5Gb"
      }
    }
  ]
}