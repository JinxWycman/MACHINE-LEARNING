{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbM+e285rqKKnAkzIMT3Eu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JinxWycman/MACHINE-LEARNING/blob/main/py_code_for_Split_algorithm_using_IDT3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zah7xj3fNDAP",
        "outputId": "48840941-4942-4fa1-d6c2-844d73bfb7ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'outlook': {'sunny': {'humidity': {'normal': True, 'high': False}}, 'overcast': True, 'rainy': {'windy': {False: True, True: False}}}}\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "# Define a toy dataset (features and labels)\n",
        "data = [\n",
        "    {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': False, 'play_tennis': False},\n",
        "    {'outlook': 'sunny', 'temperature': 'hot', 'humidity': 'high', 'windy': True, 'play_tennis': False},\n",
        "    {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'high', 'windy': False, 'play_tennis': True},\n",
        "    {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'high', 'windy': False, 'play_tennis': True},\n",
        "    {'outlook': 'rainy', 'temperature': 'cool', 'humidity': 'normal', 'windy': False, 'play_tennis': True},\n",
        "    {'outlook': 'rainy', 'temperature': 'cool', 'humidity': 'normal', 'windy': True, 'play_tennis': False},\n",
        "    {'outlook': 'overcast', 'temperature': 'cool', 'humidity': 'normal', 'windy': True, 'play_tennis': True},\n",
        "    {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'high', 'windy': False, 'play_tennis': False},\n",
        "    {'outlook': 'sunny', 'temperature': 'cool', 'humidity': 'normal', 'windy': False, 'play_tennis': True},\n",
        "    {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'normal', 'windy': False, 'play_tennis': True},\n",
        "    {'outlook': 'sunny', 'temperature': 'mild', 'humidity': 'normal', 'windy': True, 'play_tennis': True},\n",
        "    {'outlook': 'overcast', 'temperature': 'mild', 'humidity': 'high', 'windy': True, 'play_tennis': True},\n",
        "    {'outlook': 'overcast', 'temperature': 'hot', 'humidity': 'normal', 'windy': False, 'play_tennis': True},\n",
        "    {'outlook': 'rainy', 'temperature': 'mild', 'humidity': 'high', 'windy': True, 'play_tennis': False},\n",
        "]\n",
        "\n",
        "# Function to calculate entropy\n",
        "def calculate_entropy(data):\n",
        "    num_data = len(data)\n",
        "    if num_data == 0:\n",
        "        return 0.0\n",
        "    label_counts = {}\n",
        "    for record in data:\n",
        "        label = record['play_tennis']\n",
        "        if label not in label_counts:\n",
        "            label_counts[label] = 0\n",
        "        label_counts[label] += 1\n",
        "    entropy = 0.0\n",
        "    for label in label_counts:\n",
        "        prob = label_counts[label] / num_data\n",
        "        entropy -= prob * math.log(prob, 2)\n",
        "    return entropy\n",
        "\n",
        "# Function to split the dataset based on a feature\n",
        "def split_dataset(data, feature):\n",
        "    subsets = {}\n",
        "    for record in data:\n",
        "        value = record[feature]\n",
        "        if value not in subsets:\n",
        "            subsets[value] = []\n",
        "        subsets[value].append(record)\n",
        "    return subsets\n",
        "\n",
        "# Function to select the best feature to split on\n",
        "def select_best_feature(data, features):\n",
        "    base_entropy = calculate_entropy(data)\n",
        "    best_info_gain = 0.0\n",
        "    best_feature = None\n",
        "    for feature in features:\n",
        "        feature_values = set([record[feature] for record in data])\n",
        "        new_entropy = 0.0\n",
        "        for value in feature_values:\n",
        "            subset = [record for record in data if record[feature] == value]\n",
        "            prob = len(subset) / len(data)\n",
        "            new_entropy += prob * calculate_entropy(subset)\n",
        "        info_gain = base_entropy - new_entropy\n",
        "        if info_gain > best_info_gain:\n",
        "            best_info_gain = info_gain\n",
        "            best_feature = feature\n",
        "    return best_feature\n",
        "\n",
        "# Function to build the decision tree\n",
        "def build_tree(data, features):\n",
        "    class_labels = [record['play_tennis'] for record in data]\n",
        "\n",
        "    # If all instances have the same class label, return that label\n",
        "    if class_labels.count(class_labels[0]) == len(class_labels):\n",
        "        return class_labels[0]\n",
        "\n",
        "    # If there are no features left, return the majority class\n",
        "    if len(features) == 0:\n",
        "        # Handle empty data case\n",
        "        if not class_labels:\n",
        "            return None\n",
        "        return max(set(class_labels), key=class_labels.count)\n",
        "\n",
        "    # Select the best feature to split on\n",
        "    best_feature = select_best_feature(data, features)\n",
        "\n",
        "    # Create the tree structure\n",
        "    tree = {best_feature: {}}\n",
        "\n",
        "    # Get unique values for the best feature\n",
        "    feature_values = set([record[best_feature] for record in data])\n",
        "\n",
        "    # Recursively build the tree for each value of the best feature\n",
        "    for value in feature_values:\n",
        "        sub_features = features[:] # Create a copy of features\n",
        "        sub_features.remove(best_feature)\n",
        "        subset = [record for record in data if record[best_feature] == value]\n",
        "        tree[best_feature][value] = build_tree(subset, sub_features)\n",
        "\n",
        "    return tree\n",
        "\n",
        "# Main function to build and print the decision tree\n",
        "def main():\n",
        "    features = list(data[0].keys()) # List of features\n",
        "    features.remove('play_tennis') # Remove the target variable\n",
        "    tree = build_tree(data, features)\n",
        "    print(tree)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Figure 4.8: Output of code 4.5\n",
        "Split algorithm based on Gini index\n",
        "The Gini index is another common criterion for splitting in decision tree algorithms. Gini index measures the impurity of a dataset by calculating the probability of misclassifying a randomly chosen element in the dataset. The goal is to minimize the Gini index when splitting the data into subsets at each node. Code 4.6 is an example of implementing a binary decision tree split based on the Gini index in Python:\n",
        "**bold text**"
      ],
      "metadata": {
        "id": "vK1OcDmiNHhO"
      }
    }
  ]
}